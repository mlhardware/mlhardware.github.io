<!DOCTYPE html>
<html lang="en">
  <head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">

<title>IWMLH Workshop</title>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<link href="./static/skeleton.css" rel="stylesheet" type="text/css" />
<link href="./static/mlh.css" rel="stylesheet" type="text/css" />
<link href="./static/main.css" rel="stylesheet" type="text/css" />

</head>
<body>
<p> </p>
<div class="container">
<p><img class="hall" src="images/sc24.png"></p>
<h1 id="rd-international-workshop-on-machine-learning-hardware-iwmlh-co-located-with-sc-2024-in-submission">3rd International Workshop on Machine Learning Hardware (IWMLH), Co-located with SC 2024 (In Submission)</h1>
<h2 id="theme-training-and-inference-at-scale-for-large-foundation-models-fms.">Theme: Training and Inference at scale for Large Foundation Models (FMs).</h2>
<p>The 3nd International Workshop on Machine Learning Hardware is co-located
with <a href="https://sc24.supercomputing.org//">SC 2024</a>. Please scroll below
for an overview of the workshop’s scope. (Links to <a href="2020.html">first edition</a>,
<a href="2021.html">second edition</a>).</p>
<h2 id="talks">talks</h2>
<p>TBD</p>
<h3 id="workshop-scope">Workshop Scope</h3>
<p>The last decade has been marked by a race to model size in deep
learning. Industrial labs (notably OpenAI) have been training larger and
larger architectures on various kinds of tasks. This capital-intensive
endeavor has been successful in producing models that contain compressed
implementations of many fundamental language-based capabilities. We’ve
started to refer to the largest models as “Foundation Models”, in the sense that
they are a rich basis for many applications.</p>
<p>As a result, last year saw a surge of interest and use of Generative AI in the
industrial sector. There is an explosion in the training and sophistication
of closed- and open-source models, as well as in the number of providers of
inference services for these.</p>
<p>The scientific community has a particular interest in the most sophisticated
capabilities of FMs, as evidenced by the emergence of communities like
the Trillion Parameter Consortium. However, the compute requirements both
for training and inference of future trillion plus parameter models are
extreme. Accordingly, there is growing interest from the community having
its own custom hardware for both of these workload classes.</p>
<p>Previous iterations of the IWMLH workshop have focused on studying several
AI accelerator approaches to the compute requirements of ML training and
inference. Several labs have since been collaborating with AI accelerator
companies. The last iteration of this workshop focused on the applications
being developed on such experimental systems. In this iteration of the
workshop, we will focus on the question of scale. The following aspects are
of interest:</p>
<ul>
<li><strong>Characterization of large scientific FMs</strong>
Obtaining a characterization of large scientific FMs is the first step
to understanding what kind of scaled-up systems will need to be built in
order to support them. This means understanding the model architectures
of interest to the scientific community, the volume of data required
for training, and the inference requirements for running the models once
trained (throughput/speed). In other words, what uses will we want to make
of these models?</li>
<li><strong>Training</strong>
There is interest in understanding the tradeoffs of various
innovative HW approaches to building large scale systems for training. We
would like this workshop to help the scientific community understand how
innovative AI chip/system approaches constitute a step forward with respect to
the existing distributed GPU systems. The fundamentals underlying differences
in time-to-results, total cost of ownership, and energy efficiency are of
interest. This includes system architecture, networking and programming models.</li>
<li><strong>Inference</strong>
The last step of the lifecycle of large deep learning
architectures has recently garnered a lot of attention. A rich field has
emerged relating to the Deployment and Inference of the largest models, with
techniques such as Batching, Quantization, Layer Fusion being used today. We
would like this workshop to help the scientific community understand the new
tradeoffs that AI chip/system approaches offer. Of particular interest are
the fundamentals driving differences in the total cost of ownership, energy
efficiency, and various metrics relating to inference throughput and speed.</li>
</ul>
<h3 id="organizing-committee">Organizing Committee</h3>
<p>Charlie Catlett, Argonne National Laboratory - <code>catlett@anl.gov</code><br />
Swann Perarnau, Argonne National Laboratory - <code>swann@anl.gov</code><br />
Valentin Reis, Groq, Inc - <code>vreis@groq.com</code></p>
<h3 id="steering-committe">Steering Committe</h3>
<p>Takano Ryousei, AIST<br />
Pete Beckman, ANL<br />
Kentaro Sano, RIKEN<br />
Prasanna Balaprakash, ORNL<br />
Rosa M. Badia, BSC<br />
Tony Hey, STFC UKRI<br />
Haohuan Fu, Tsinghua University/NSCCWX</p>
</div></body>
</html>
